---
title: "Confirmatory Factor Analysis with a Small Sample"
author: "Juan C. Correa"
date: "2024-01-06"
output: html_document
---

# Context

CFA is a well-known technique in psychology and social sciences. Despite its reputation, like any other technique, it also has its limitations. For example, in educational research, it is quite common to have small sample size (few observations) if they are collected through survey-based techniques such as scales or questionnaires. Our following case builds upon the context of a COIL experience where 65 students (i.e., 33 from Colombia and 32 from Ecuador) got together during six weeks for a two-hour-sessions per week using Google Classroom. Let's open the data an conduct a standard Confirmatory Factor Analysis. 

The data relates to 15 items (observed variables) which are theoretically linked to the concept of Transactive Memory System (TMS). TMS refers to the way people work in collaboration with others. Thus, TMS can be regarded as a "Collaboration Skill". Here, we want to test the psychometric structure of the scale originally developed for English-speakers by Lewis (2003) and re-adapted for Spanish by García-Chitiva (2021).

```{r}
library(readr)
coildata <- coildata <- read_csv("coildata.csv")
```


## Confirmatory Factor Analysis

Our first "naïve" approach is to specify a full confirmatory factor analysis model with the following three latent variables of TMS

- SP (Specialization)
- CR (Credibility), and
- CD (Coordination)

```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- coildata ;
 model<-"
! regressions 
   SP=~SP__SP1*SP1
   SP=~SP__SP2*SP2
   SP=~SP__SP3*SP3
   SP=~SP__SP4*SP4
   SP=~SP__SP5*SP5
   CR=~CR__CR1*CR1
   CR=~CR__CR2*CR2
   CR=~CR__CR3*CR3
   CR=~CR__CR4*CR4
   CR=~CR__CR5*CR5
   CD=~CD__CD1*CD1
   CD=~CD__CD2*CD2
   CD=~CD__CD3*CD3
   CD=~CD__CD4*CD4
   CD=~CD__CD5*CD5
! residuals, variances and covariances
   SP1 ~~ VAR_SP1*SP1
   SP2 ~~ VAR_SP2*SP2
   SP3 ~~ VAR_SP3*SP3
   SP4 ~~ VAR_SP4*SP4
   SP5 ~~ VAR_SP5*SP5
   CR1 ~~ VAR_CR1*CR1
   CR2 ~~ VAR_CR2*CR2
   CR3 ~~ VAR_CR3*CR3
   CR4 ~~ VAR_CR4*CR4
   CR5 ~~ VAR_CR5*CR5
   CD1 ~~ VAR_CD1*CD1
   CD2 ~~ VAR_CD2*CD2
   CD3 ~~ VAR_CD3*CD3
   CD4 ~~ VAR_CD4*CD4
   CD5 ~~ VAR_CD5*CD5
   SP ~~ 1.0*SP
   CR ~~ 1.0*CR
   CD ~~ 1.0*CD
   SP ~~ COV_SP_CR*CR
   CR ~~ COV_CR_CD*CD
   SP ~~ COV_SP_CD*CD
! observed means
   SP1~1;
   SP2~1;
   SP3~1;
   SP4~1;
   SP5~1;
   CR1~1;
   CR2~1;
   CR3~1;
   CR4~1;
   CR5~1;
   CD1~1;
   CD2~1;
   CD3~1;
   CD4~1;
   CD5~1;
"
result1<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result2<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
```

Before showing the goodness-of-fit for this model, we highlight its standardized statistical estimates, so we can spot which items should be removed due to their low association with the latent variables.


```{r}
library(semPlot)
semPaths(result2, whatLabels = "std", layout = "spring", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 1,
  node.width = 1,
  node.height = 1,
  mar = c(1, 1, 1, 1), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```

It should be evident that the following items didn't behave as expected:

- CR4
- CR5
- SP2
- CD3
- CD5

Before removing these items from our original specification, we want to highlight the model's fit through the maximum likelihood (ML) method with (result1) and without (result2) the Satorra-Bentler adjustment, as follows:


```{r}
fit <- summary(result2, fit.measures=TRUE)
fit$fit[3:5]
fit$fit[6:8]
fit$fit[17:22]
fit$fit[29:33]
fit$fit[37:40]
fit$fit[42:45]
fit$fit[47]
```

In SEM, goodness-of-fit indices such as CFI and TLI should be higher than 0.9 to claim that the model reveals a good fit—likewise, RMSEA is lower than 0.05, and SRMR is lower than 0.08. Interested readers can consult the article of Hu and Bentler (1999) to dive deep into the rationality of these rules of thumb.





```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- read_delim("coil.csv", delim = ";", 
    escape_double = FALSE, trim_ws = TRUE)
 model<-"
! regressions 
   DS=~x2__Instrumental*IN
   DS=~x2__Stra_Privilege*PR
   DS=~x2__Stra_Appropriation*AP
   DS=~x2__Expansive*EX
! residuals, variances and covariances
   IN ~~ VAR_Instrumental*IN
   PR ~~ VAR_Stra_Privilege*PR
   AP ~~ VAR_Stra_Appropriation*AP
   EX ~~ VAR_Expansive*EX
   DS ~~ 1.0*DS
! observed means
   IN~1;
   PR~1;
   AP~1;
   EX~1;
";
```

The results of this "abbreviated" are as follows:

```{r}
result3<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result4<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
summary(result4, fit.measures=TRUE)
```



A graphical output of "result4" should look like this:

```{r}
library(semPlot)
semPaths(result4, whatLabels = "std", layout = "tree", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 2,
  node.width = 2,
  node.height = 2,
  mar = c(10, 5, 10, 5), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```



To inspect the standardized estimates for factor loadings
```{r}
lavInspect(result4, what = "std.all")
```


# The bad news

The bad news about the "naïve" and "abbreviated" confirmatory factor analyses is that the goodness-of-fit reported for both of these models are misleading because they are the result of an improper solution that is visible when we inspect the estimated variance-covariance matrix for all observed and latent variables.

```{r, echo = TRUE}
m1 <- lavInspect(result2, what = "vcov.std.all")
eigen(m1)
m2 <- lavInspect(result4, what = "vcov.std.all")
eigen(m2)
```

The definitive proof that this solution is not admissible is evident if you run the Cholesky decomposition by running `chol(m1)` for the "naïve" model and `chol(m2)` for the "abbreviated" model.

