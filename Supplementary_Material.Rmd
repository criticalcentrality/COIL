---
title: "Confirmatory Factor Analysis with a Small Sample"
author: "Juan C. Correa"
date: "2024-01-06"
output: html_document
---

# Context

CFA is a well-known technique in psychology and social sciences. Despite its reputation, like any other technique, it also has its limitations. For example, in educational research, it is quite common to have small sample size (few observations) if they are collected through survey-based techniques such as scales or questionnaires. Our following case builds upon the context of a COIL experience where 65 students (i.e., 33 from Colombia and 32 from Ecuador) got together during six weeks for a two-hour-sessions per week using Google Classroom. Let's open the data an conduct a standard Confirmatory Factor Analysis. 

The data relates to 15 items (observed variables) which are theoretically linked to the concept of Transactive Memory System (TMS). TMS refers to the way people work in collaboration with others. Thus, TMS can be regarded as a "Collaboration Skill". Here, we want to test the psychometric structure of the scale originally developed for English-speakers by Lewis (2003) and re-adapted for Spanish by García-Chitiva (2021).

```{r}
library(readr)
coildata <- coildata <- read_csv("coildata.csv")
```

## Testing Univariate and Multivariate Normal Distribution Assumptions

The evaluation of these assumptions is relatively straightforward through exploratory data visualization of kernel density plots and the Henze-Zirkler test implemented in the R package MVN

```{r}
library(ggplot2)

coildata_long <- reshape2::melt(coildata)

ggplot(coildata_long, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~variable, scales = "free", ncol = 5) +
  theme_minimal() +  # Optional: Customize the theme
  labs(title = "Kernel Density Plots for Coildata Variables")

library(MVN)
mvn(coildata)
```


## Confirmatory Factor Analysis

Our first measurement model specification can be regarded as a confirmatory factor analysis, as it sets the full theoretical factor structure with the following three latent variables: Specialization (SP), Credibility (CR), and Coordination (CD) with 15 observed variables.

### A First Model

```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- coildata ;
 model<-"
! regressions 
   SP=~SP__SP1*SP1
   SP=~SP__SP2*SP2
   SP=~SP__SP3*SP3
   SP=~SP__SP4*SP4
   SP=~SP__SP5*SP5
   CR=~CR__CR1*CR1
   CR=~CR__CR2*CR2
   CR=~CR__CR3*CR3
   CR=~CR__CR4*CR4
   CR=~CR__CR5*CR5
   CD=~CD__CD1*CD1
   CD=~CD__CD2*CD2
   CD=~CD__CD3*CD3
   CD=~CD__CD4*CD4
   CD=~CD__CD5*CD5
! residuals, variances and covariances
   SP1 ~~ VAR_SP1*SP1
   SP2 ~~ VAR_SP2*SP2
   SP3 ~~ VAR_SP3*SP3
   SP4 ~~ VAR_SP4*SP4
   SP5 ~~ VAR_SP5*SP5
   CR1 ~~ VAR_CR1*CR1
   CR2 ~~ VAR_CR2*CR2
   CR3 ~~ VAR_CR3*CR3
   CR4 ~~ VAR_CR4*CR4
   CR5 ~~ VAR_CR5*CR5
   CD1 ~~ VAR_CD1*CD1
   CD2 ~~ VAR_CD2*CD2
   CD3 ~~ VAR_CD3*CD3
   CD4 ~~ VAR_CD4*CD4
   CD5 ~~ VAR_CD5*CD5
   SP ~~ 1.0*SP
   CR ~~ 1.0*CR
   CD ~~ 1.0*CD
   SP ~~ COV_SP_CR*CR
   CR ~~ COV_CR_CD*CD
   SP ~~ COV_SP_CD*CD
! observed means
   SP1~1;
   SP2~1;
   SP3~1;
   SP4~1;
   SP5~1;
   CR1~1;
   CR2~1;
   CR3~1;
   CR4~1;
   CR5~1;
   CD1~1;
   CD2~1;
   CD3~1;
   CD4~1;
   CD5~1;
"
result1<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result2<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
```

Before showing the goodness-of-fit for this model, we highlight its standardized statistical estimates, so we can spot which items should be removed due to their low association with the latent variables.


```{r}
library(semPlot)
semPaths(result2, whatLabels = "std", layout = "spring", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 1,
  node.width = 1,
  node.height = 1,
  mar = c(1, 1, 1, 1), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```

It should be evident that the following items didn't behave as expected:

- CR4
- CR5
- SP2
- CD3
- CD5

Before removing these items from our original specification, we want to highlight the model's fit through the maximum likelihood (ML) method with (result1) and without (result2) the Satorra-Bentler adjustment, as follows:


```{r}
fit <- summary(result2, fit.measures=TRUE)
fit$fit[3:5]
fit$fit[6:8]
fit$fit[17:22]
fit$fit[29:33]
fit$fit[37:40]
fit$fit[42:45]
fit$fit[47]
```

In SEM, goodness-of-fit indices such as CFI and TLI should be higher than 0.9 to claim that the model reveals a good fit—likewise, RMSEA is lower than 0.05, and SRMR is lower than 0.08. Interested readers can consult the article of Hu and Bentler (1999) to dive deep into the rationality of these rules of thumb.  

### A Second Model

Our data-driven approach at this moment, can help newcomers to anticipate that our original model should outperform if we removed the items that didn't behave as expected. Let's see if this is the case.

```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- coildata
 model2<-"
! regressions 
   SP=~SP__SP1*SP1
   SP=~SP__SP3*SP3
   SP=~SP__SP4*SP4
   SP=~SP__SP5*SP5
   CR=~CR__CR1*CR1
   CR=~CR__CR2*CR2
   CR=~CR__CR3*CR3
   CD=~CD__CD1*CD1
   CD=~CD__CD2*CD2
   CD=~CD__CD4*CD4
! residuals, variances and covariances
   SP1 ~~ VAR_SP1*SP1
   SP3 ~~ VAR_SP3*SP3
   SP4 ~~ VAR_SP4*SP4
   SP5 ~~ VAR_SP5*SP5
   CR1 ~~ VAR_CR1*CR1
   CR2 ~~ VAR_CR2*CR2
   CR3 ~~ VAR_CR3*CR3
   CD1 ~~ VAR_CD1*CD1
   CD2 ~~ VAR_CD2*CD2
   CD4 ~~ VAR_CD4*CD4
   SP ~~ 1.0*SP
   CR ~~ 1.0*CR
   CD ~~ 1.0*CD
   SP ~~ COV_SP_CR*CR
   CR ~~ COV_CR_CD*CD
   SP ~~ COV_SP_CD*CD
! observed means
   SP1~1;
   SP3~1;
   SP4~1;
   SP5~1;
   CR1~1;
   CR2~1;
   CR3~1;
   CD1~1;
   CD2~1;
   CD4~1;
";
result3<-lavaan(model2, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result4<-lavaan(model2, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
```

The results of this second model look as follows:

```{r}
library(semPlot)
semPaths(result4, whatLabels = "std", layout = "spring", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 1,
  node.width = 1,
  node.height = 1,
  mar = c(1, 1, 1, 1), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```

Now, let's check the results of this second model

```{r}
fit2 <- summary(result4, fit.measures=TRUE)
fit2$fit[3:5]
fit2$fit[6:8]
fit2$fit[17:22]
fit2$fit[29:33]
fit2$fit[37:40]
fit2$fit[42:45]
fit2$fit[47]
```


At this moment, a practitioner might be tempted to claim that this model proved to be satisfactory from a statistical viewpoint, given its following results:

- All standardized estimates for factor loadings are high and statistically significant
- The covariance between latent variables are high and statistically significant
- The goodness-of-fit indices fulfil the rules of thumb suggested by Hu and Bentler (1999)

Even though all of these results are promising, we will show that they should not be taken for granted, due to the existence of improper solutions in the estimation process. To conduct a final check (often not recognized by practitioners) a good strategy might be to look at the estimated variance-covariance matrix for all observed and latent variables. This estimated matrix is $\hat{\theta}_\delta$ from the general equation introduced by Bollen (1989) $\Sigma = \Sigma(\hat{\theta}_\delta)$.

To inspect the standardized estimates for factor loadings


## The bad news about ML

The bad news about the "first" and "second" confirmatory factor analyses is that the goodness-of-fit reported for both of these models are misleading even if they look okay because they are the result of an improper solution that is visible when we inspect the estimated variance-covariance matrix for all observed and latent variables.


```{r, echo = TRUE}
m1 <- lavInspect(result2, what = "vcov.std.all")
EIGEN <- eigen(m1)
EIGEN$values
m2 <- lavInspect(result4, what = "vcov.std.all")
EIGEN2 <- eigen(m2)
EIGEN2$values
```

The definitive proof that this solution is not admissible is evident if you run the Cholesky decomposition by running `chol(m1)` for the "first" model and `chol(m2)` for the "second" model.

# Beyond Maximum Likelihood

Because the strategy of maximum likelihood estimation method was the common element between these two models, then we need to use another estimation method. An alternative is known as "Model-implied instrumental variables." In this approach, the model is translated to a set of (regression)
equations. Next, each latent variable in these equations is replaced with its marker indicator (usually the ﬁrst indicator, where the factor loading is ﬁxed to unity and the intercept is ﬁxed to zero) minus its residual error term. The resulting equations no longer contain any latent variables but have a more complex error structure.

Importantly, ordinary least squares estimation is no longer suitable for solving these equations because some predictors are now correlated with the error term in the equation. This is where the instrumental variables (also called instruments) come into play. For each equation, a set of instrumental variables must be found. An instrumental variable must be uncorrelated with the error term of the equation but strongly correlated with the problematic predictor. Usually, instrumental variables are sought outside the model, but in Bollen’s approach, the instrumental variables are selected from the observed variables that are part of the model. Several (automated) procedures to ﬁnd these instrumental variables within the model have been developed. Once the instruments are selected, an estimation procedure is needed to estimate all the coeﬃcients of the equations. Econometricians developed a popular method to accomplish this called two-stage least squares (2SLS).

```{r}
library(MIIVsem)
miivs(model)
summary(miivs(model), eq.info = TRUE)
pave <- miive(model, coildata)
estimatesTable(pave)
```

