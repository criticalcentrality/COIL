---
title: "Confirmatory Factor Analysis with a Small Sample"
author: "Juan C. Correa"
date: "2024-01-06"
output: html_document
---

# Context

CFA is a well-known technique in psychology and social sciences. Despite its reputation, like any other technique, it also has its limitations. For example, in educational research, it is quite common to have small sample size (few observations) if they are collected through survey-based techniques such as scales or questionnaires. Our following case builds upon the context of a COIL experience where 65 students (i.e., 33 from Colombia and 32 from Ecuador) got together during six weeks for a two-hour-sessions per week using Google Classroom. Let's open the data an conduct a standard Confirmatory Factor Analysis. 

```{r}
library(readr)
coildata <- coildata <- read_csv("coildata.csv")
```


## Confirmatory Factor Analysis

Our first "naïve" approach is to specify a full confirmatory factor analysis model with the following four latent variables which are theoretically linked with the concept of "digital skills":

- EX (Expansive use of digital tools)
- IN (Instrumental use of digital tools)
- AP (Strategic appropriation use of digital tools), and
- PR (Strategic privileged use of digital tools)

```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- coildata;
 model<-"
! regressions 
   EX=~EX__EX1*EX1
   EX=~EX__EX2*EX2
   EX=~EX__EX3*EX3
   EX=~EX__EX4*EX4
   EX=~EX__EX5*EX5
   IN=~IN__IN1*IN1
   IN=~IN__IN2*IN2
   IN=~IN__IN3*IN3
   IN=~IN__IN4*IN4
   IN=~IN__IN5*IN5
   IN=~IN__IN6*IN6
   AP=~AP__AP1*AP1
   AP=~AP__AP2*AP2
   AP=~AP__AP3*AP3
   AP=~AP__AP4*AP4
   PR=~PR__PR1*PR1
   PR=~PR__PR2*PR2
   PR=~PR__PR3*PR3
   PR=~PR__PR4*PR4
   PR=~PR__PR5*PR5
   PR=~PR__PR6*PR6
   PR=~PR__PR7*PR7
! residuals, variances and covariances
   IN ~~ 1.0*IN
   PR ~~ 1.0*PR
   AP ~~ 1.0*AP
   EX ~~ 1.0*EX
   EX ~~ COV_EX_IN*IN
   EX ~~ COV_EX_PR*PR
   EX ~~ COV_EX_AP*AP
   IN ~~ COV_IN_PR*PR
   IN ~~ COV_IN_AP*AP
   AP ~~ COV_AP_PR*PR
   EX1 ~~ VAR_EX1*EX1
   EX2 ~~ VAR_EX2*EX2
   EX3 ~~ VAR_EX3*EX3
   EX4 ~~ VAR_EX4*EX4
   EX5 ~~ VAR_EX5*EX5
   IN1 ~~ VAR_IN1*IN1
   IN2 ~~ VAR_IN2*IN2
   IN3 ~~ VAR_IN3*IN3
   IN4 ~~ VAR_IN4*IN4
   IN5 ~~ VAR_IN5*IN5
   IN6 ~~ VAR_IN6*IN6
   AP1 ~~ VAR_AP1*AP1
   AP2 ~~ VAR_AP2*AP2
   AP3 ~~ VAR_AP3*AP3
   AP4 ~~ VAR_AP4*AP4
   PR1 ~~ VAR_PR1*PR1
   PR2 ~~ VAR_PR2*PR2
   PR3 ~~ VAR_PR3*PR3
   PR4 ~~ VAR_PR4*PR4
   PR5 ~~ VAR_PR5*PR5
   PR6 ~~ VAR_PR6*PR6
   PR7 ~~ VAR_PR7*PR7
! observed means
   EX1~1;
   EX2~1;
   EX3~1;
   EX4~1;
   EX5~1;
   IN1~1;
   IN2~1;
   IN3~1;
   IN4~1;
   IN5~1;
   IN6~1;
   AP1~1;
   AP2~1;
   AP3~1;
   AP4~1;
   PR1~1;
   PR2~1;
   PR3~1;
   PR4~1;
   PR5~1;
   PR6~1;
   PR7~1;
";
```

The results of this model specification are estimated under maximum likelihood (ML) estimation method with (result1) and without (result2) the Satorra-Bentler adjustment, as follows:
```{r}
result1<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result2<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
summary(result2, fit.measures=TRUE)
```

The visual appearance of this full model is as follows:

```{r}
library(semPlot)
semPaths(result2, whatLabels = "std", layout = "spring", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 1,
  node.width = 1,
  node.height = 1,
  mar = c(1, 1, 1, 1), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```




```{r}
#
# This model specification was automatically generated by Onyx
#
library(lavaan);
modelData <- read_delim("coil.csv", delim = ";", 
    escape_double = FALSE, trim_ws = TRUE)
 model<-"
! regressions 
   DS=~x2__Instrumental*IN
   DS=~x2__Stra_Privilege*PR
   DS=~x2__Stra_Appropriation*AP
   DS=~x2__Expansive*EX
! residuals, variances and covariances
   IN ~~ VAR_Instrumental*IN
   PR ~~ VAR_Stra_Privilege*PR
   AP ~~ VAR_Stra_Appropriation*AP
   EX ~~ VAR_Expansive*EX
   DS ~~ 1.0*DS
! observed means
   IN~1;
   PR~1;
   AP~1;
   EX~1;
";
```

The results of this "abbreviated" are as follows:

```{r}
result3<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="ML", std.ov=TRUE);
result4<-lavaan(model, data=modelData, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
summary(result4, fit.measures=TRUE)
```



A graphical output of "result4" should look like this:

```{r}
library(semPlot)
semPaths(result4, whatLabels = "std", layout = "tree", color = list(
  lat = rgb(124, 12, 199, maxColorValue = 255),
  man = rgb(155, 253, 175, maxColorValue = 255)),
  edge.color = "black",
  edge.label.cex = 1,
  edge.width = 1.5,
  label.cex = 2,
  node.width = 2,
  node.height = 2,
  mar = c(10, 5, 10, 5), intercepts = FALSE, residuls = FALSE, nCharNodes = 0)
```



To inspect the standardized estimates for factor loadings
```{r}
lavInspect(result4, what = "std.all")
```


# The bad news

The bad news about the "naïve" and "abbreviated" confirmatory factor analyses is that the goodness-of-fit reported for both of these models are misleading because they are the result of an improper solution that is visible when we inspect the estimated variance-covariance matrix for all observed and latent variables.

```{r, echo = TRUE}
m1 <- lavInspect(result2, what = "vcov.std.all")
eigen(m1)
m2 <- lavInspect(result4, what = "vcov.std.all")
eigen(m2)
```

The definitive proof that this solution is not admissible is evident if you run the Cholesky decomposition by running `chol(m1)` for the "naïve" model and `chol(m2)` for the "abbreviated" model.

