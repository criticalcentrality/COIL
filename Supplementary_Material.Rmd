---
title: "Confirmatory Factor Analysis with a Small Sample"
author: "Juan C. Correa"
date: "2024-01-06"
output: html_document
---

# Context

CFA is a well-known technique in psychology and social sciences. Despite its reputation, like any other technique, it also has its limitations. For example, in educational research, it is quite common to have small sample size (few observations) if they are collected through survey-based techniques such as scales or questionnaires. Our following case builds upon the context of a COIL experience where 65 students (i.e., 33 from Colombia and 32 from Ecuador) got together during six weeks for a two-hour-sessions per week using Google Classroom. Let's open the data an conduct a standard Confirmatory Factor Analysis. 

The data relates to 15 items (observed variables) which are theoretically linked to the concept of Transactive Memory System (TMS). TMS refers to the way people work in collaboration with others. Thus, TMS can be regarded as a "Collaboration Skill". Here, we want to test the psychometric structure of the scale originally developed for English-speakers by Lewis (2003) and re-adapted for Spanish by Garc√≠a-Chitiva (2021).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readr)
coildata <- coildata <- read_csv("coildata.csv")
```

## Testing Univariate and Multivariate Normal Distribution Assumptions

The evaluation of these assumptions is relatively straightforward through exploratory data visualization of kernel density plots and the Henze-Zirkler test implemented in the R package MVN

```{r}
library(ggplot2)

coildata_long <- reshape2::melt(coildata)

ggplot(coildata_long, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~variable, scales = "free", ncol = 5) +
  theme_minimal() +  # Optional: Customize the theme
  labs(title = "Statistical distributions of observed variables")

library(MVN)
mvn(coildata)
```

## In-Depth Exploration Strategies

We will conduct our analyses by following two general strategies: "Bottom-up" and "Top-Down." In the "Bottom-up" strategy, we will develop our measurement scale by testing each of its theoretical dimensions (one at time). In the "Top-Down" strategy we go the other way around. 

## Bottom-up: Specialization Scale

Let's see how our model should look like.

```{r}
library(lavaan);
modelData <- coildata
model<-"
! regressions 
   SP=~SP__SP1*SP1
   SP=~SP__SP2*SP2
   SP=~SP__SP3*SP3
   SP=~SP__SP4*SP4
   SP=~SP__SP5*SP5
! residuals, variances and covariances
   SP1 ~~ VAR_SP1*SP1
   SP2 ~~ VAR_SP2*SP2
   SP3 ~~ VAR_SP3*SP3
   SP4 ~~ VAR_SP4*SP4
   SP5 ~~ VAR_SP5*SP5
   SP ~~ 1.0*SP
! observed means
   SP1~1;
   SP2~1;
   SP3~1;
   SP4~1;
   SP5~1;
";
result1 <- lavaan(model, data=coildata, estimator="ML", std.ov=TRUE);
summary(result1, fit.measures=TRUE);

result2<-lavaan(model, data=coildata, fixed.x=FALSE, estimator="MLM", std.ov = TRUE);
summary(result2, fit.measures=TRUE);

fit2 <- summary(result2, fit.measures=TRUE)
fit1 <- summary(result1, fit.measures=TRUE)

lavTest(result1, test = "browne.residual.adf", output = "text")
lavTest(result2, test = "browne.residual.adf", output = "text")
```


```{r, echo=FALSE}
Fit.Index <- c("Chi2", "CFI", "TLI", "RMSEA", "SRMR", "AIC", "BIC")
ModelA <- c(fit1$fit[3], fit1$fit[9],  fit1$fit[10], fit1$fit[17], fit1$fit[25], fit1$fit[13], fit1$fit[14])
ModelB <- c(fit2$fit[6], fit2$fit[21], fit2$fit[22], fit2$fit[42], fit2$fit[47], fit2$fit[25], fit2$fit[26])
Results <- data.frame(Fit.Index, ModelA, ModelB)
Results <- round(Results[2:3], 3)
Results$Fit.Index <- c("Chi2", "CFI", "TLI", "RMSEA", "SRMR", "AIC", "BIC")
Results <- Results[, c(3, 1, 2)]
library(kableExtra)
tbl <- kable(Results)
tbl <- tbl %>%
  kable_styling(full_width = FALSE)
tbl
```



Our first measurement model specification can be regarded as a confirmatory factor analysis, as it sets the full theoretical factor structure with the following three latent variables: Specialization (SP), Credibility (CR), and Coordination (CD) with 15 observed variables.

